import residual_learning.residual_sac
import residual_learning.networks
import residual_learning.state_machines
import residual_learning.make_training_env
import cpc.state_machine
import cic.states.custom_state_machines

train.algorithm = @ResidualSAC
train.maxt = 1000000
train.seed = 0
train.eval = True
train.eval_period = 100000
train.save_period = 10000
train.maxseconds = None

optim.Adam.betas = (0.9, 0.999)


max_torque = 0.1


NetworkParams.size = 256
NetworkParams.embedding_size = 64
NetworkParams.max_torque = %max_torque
NetworkParams.pi_layers = 2
NetworkParams.vf_layers = 3
NetworkParams.init_std = 1.0

Checkpointer.ckpt_period = 10000

ResidualSAC.policy_training_start = 40000
ResidualSAC.policy_zero_end = 100000
ResidualSAC.env_fn = @make_training_env
ResidualSAC.policy_fn = @policy_fn
ResidualSAC.qf_fn = @qf_fn
ResidualSAC.nenv = 1
ResidualSAC.eval_num_episodes = 20
ResidualSAC.record_num_episodes = 0
ResidualSAC.buffer_size = 1000000
ResidualSAC.frame_stack = 1
ResidualSAC.learning_starts = 20000
ResidualSAC.update_period = 1
ResidualSAC.gpu = True
ResidualSAC.optimizer = @optim.Adam
ResidualSAC.batch_size = 256
ResidualSAC.policy_lr = 3e-4
ResidualSAC.qf_lr = 3e-4
ResidualSAC.gamma = 0.99
ResidualSAC.target_update_period = 1
ResidualSAC.policy_update_period = 4
ResidualSAC.alpha = 0.2
ResidualSAC.target_smoothing_coef = 0.001
ResidualSAC.automatic_entropy_tuning = True
ResidualSAC.target_entropy = None
ResidualSAC.log_period = 100

make_training_env.goal_difficulty = 3
make_training_env.action_space = "torque_and_position"
make_training_env.frameskip = 3
make_training_env.visualization = False
make_training_env.reward_fn = "gaussian_reward"
make_training_env.termination_fn = "no_termination"
make_training_env.initializer = "training_init"
make_training_env.episode_length = 1000
make_training_env.monitor = False
make_training_env.seed = 0
make_training_env.domain_randomization = False
make_training_env.norm_observations = True
make_training_env.state_machine = @MPPGStateMachine
make_training_env.max_torque = %max_torque
make_training_env.residual_state = 'MoveToGoalState'

VecObsNormWrapper.steps = 10000
VecObsNormWrapper.mean = None
VecObsNormWrapper.std = None
VecObsNormWrapper.eps = 1e-2
VecObsNormWrapper.log = True
VecObsNormWrapper.log_prob = 0.01
